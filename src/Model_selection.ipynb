{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../data_processed/clean/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('../data_processed/clean/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('Class', axis=1)\n",
    "y_train = train['Class']\n",
    "X_test = test.drop('Class', axis=1)\n",
    "y_test = test['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de distintos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular métricas tras la predicción\n",
    "def classification_metrics(y_test, y_pred, name, model=np.NaN, roc_bool=True):\n",
    "    \"\"\"\n",
    "    Calcula y muestra diversas métricas de clasificación tras realizar una predicción.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    y_test : array-like\n",
    "        Valores verdaderos de la variable objetivo (labels) del conjunto de test.\n",
    "    y_pred : array-like\n",
    "        Valores predichos por el modelo.\n",
    "    name : str\n",
    "        Nombre o descripción del modelo, utilizado para etiquetar la salida.\n",
    "    model : object, optional\n",
    "        Modelo utilizado para hacer la predicción. Se utiliza para calcular la métrica\n",
    "        ROC-AUC si el modelo soporta la función `predict_proba()`. Por defecto, es np.NaN.\n",
    "    roc_bool : bool, optional\n",
    "        Variable para pintar (o no) la curva ROC-AUC.\n",
    "\n",
    "    Métricas calculadas:\n",
    "    --------------------\n",
    "    - Accuracy: Precisión del modelo.\n",
    "    - Matriz de confusión: Representación de los aciertos y errores del modelo.\n",
    "    - Reporte de clasificación: Incluye precisión, recall, F1-score, etc.\n",
    "    - ROC-AUC: Se calcula solo si el modelo soporta `predict_proba()`.\n",
    "    \n",
    "    Devuelve:\n",
    "    ---------\n",
    "    data_metrics : pd.DataFrame\n",
    "        Dataframe con la información de métricas del modelo evaluado en la función.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcular la precisión (accuracy)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\n----- {name} -----\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    metrics_summary = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': [], 'ROC-AUC': []}\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calcular y mostrar ROC-AUC si el modelo lo permite y se pide\n",
    "    if roc_bool:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)[:,1]\n",
    "            roc_auc = roc_auc_score(y_test, y_proba)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Aleatorio')\n",
    "            plt.xlabel('Tasa de Falsos Positivos')\n",
    "            plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "            plt.title('Curva ROC')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        else:\n",
    "            print(f\"ROC-AUC: No disponible para {name}\")\n",
    "\n",
    "     # Almacenar los resultados en el diccionario\n",
    "    metrics_summary['Model'].append(name)\n",
    "    metrics_summary['Accuracy'].append(accuracy)\n",
    "    metrics_summary['Precision'].append(precision)\n",
    "    metrics_summary['Recall'].append(recall)\n",
    "    metrics_summary['F1-Score'].append(f1)\n",
    "    if roc_bool == True & hasattr(model, \"predict_proba\"):\n",
    "        metrics_summary['ROC-AUC'].append(roc_auc)\n",
    "    else:\n",
    "        metrics_summary['ROC-AUC'].append(np.NaN)\n",
    "    \n",
    "    return pd.DataFrame(metrics_summary)\n",
    "\n",
    "# Función para entrenar y evaluar modelos\n",
    "def evaluate_models(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa múltiples modelos de clasificación en un conjunto de datos de prueba.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Un diccionario donde las claves son los nombres de los modelos (como cadenas de texto) \n",
    "        y los valores son instancias de los modelos de clasificación.\n",
    "    X_train : array-like\n",
    "        Características (features) del conjunto de entrenamiento.\n",
    "    X_test : array-like\n",
    "        Características (features) del conjunto de prueba.\n",
    "    y_train : array-like\n",
    "        Valores verdaderos de la variable objetivo (labels) del conjunto de entrenamiento.\n",
    "    y_test : array-like\n",
    "        Valores verdaderos de la variable objetivo (labels) del conjunto de prueba.\n",
    "\n",
    "    Funcionalidad:\n",
    "    --------------\n",
    "    - Entrena cada modelo en `models` usando el conjunto de entrenamiento `X_train` y `y_train`.\n",
    "    - Predice los valores de la variable objetivo en `X_test` para cada modelo.\n",
    "    - Calcula y muestra métricas de clasificación (incluyendo accuracy, matriz de confusión, \n",
    "      reporte de clasificación y ROC-AUC si está disponible) utilizando la función \n",
    "      `classification_metrics`.\n",
    "\n",
    "    Devuelve:\n",
    "    ---------\n",
    "    data_metrics : pd.DataFrame\n",
    "        Dataframe con la información de métricas de cada modelo evaluado en la función.\n",
    "    \"\"\"\n",
    "\n",
    "    data_metrics = pd.DataFrame()\n",
    "    \n",
    "    # Iterar sobre todos los modelos definidos\n",
    "    for name, model in models.items():\n",
    "       \n",
    "        # Entrenar el modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir en los datos de prueba\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Sacar métricas\n",
    "        df_met_model = classification_metrics(y_test, y_pred, name, model)\n",
    "\n",
    "        # Métricas training\n",
    "        print(\"--- MÉTRICAS CON ENTRENAMIENTO: ---\")\n",
    "        df_met_model_train = classification_metrics(y_train, model.predict(X_train), name, model, roc_bool=False)\n",
    "\n",
    "        # Renombrar columnas de df_met_model_train para concatenar\n",
    "        df_met_model_train = df_met_model_train.drop(columns=[\"ROC-AUC\"])\n",
    "        df_met_model_train.rename(columns={col: col + '_train' for col in df_met_model_train.columns if col != 'Model'}, inplace=True)\n",
    "        \n",
    "        # Juntar los dos dataframes en uno\n",
    "        df_met_model = df_met_model.merge(df_met_model_train, left_on='Model', right_on='Model')\n",
    "        \n",
    "        if data_metrics.shape[0] > 0:\n",
    "            data_metrics = pd.concat([data_metrics, df_met_model])\n",
    "        else:\n",
    "            data_metrics = df_met_model\n",
    "\n",
    "    return data_metrics.reset_index(drop=True).loc[:, ['Model', 'Accuracy_train', 'Accuracy', 'Precision_train', 'Precision', 'Recall_train', 'Recall', 'F1-Score_train', 'F1-Score', 'ROC-AUC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agg_pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
